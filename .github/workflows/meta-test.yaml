name: üõ°Ô∏è Meta-Test Governance Logic

on:
  # Trigger on every push to any branch
  push:

  # Trigger on every PR to any branch
  pull_request:

  # Optional: Keep manual trigger for easy testing
  workflow_dispatch:

jobs:
  test-governance-logic:
    runs-on: ubuntu-latest
    steps:
      # ---------------------------------------------------------
      # 1. Checkout the Hub Repo
      # ---------------------------------------------------------
      - name: Checkout Hub
        uses: actions/checkout@v4

      # ---------------------------------------------------------
      # 2. Setup Environment
      # ---------------------------------------------------------
      - name: Setup Node
        uses: actions/setup-node@v4
        with: { node-version: '18' }
      - name: Setup Python
        uses: actions/setup-python@v4
        with: { python-version: '3.x' }

      # ---------------------------------------------------------
      # 3. Install Dependencies
      # ---------------------------------------------------------
      - name: Install Dependencies
        run: |
          npm install openapi-diff @stoplight/spectral-cli
          pip install PyYAML

      # ========================================================
      # SCENARIO: ROBUST MIX (The Ultimate Test)
      # Tests 8 specific governance scenarios:
      # 1. Legacy API unchanged (GET /pet) -> IGNORE
      # 2. Legacy API modified (GET /pet/findByStatus) -> IGNORE
      # 3. Legacy API new method (PATCH /pet/findByStatus) -> INCLUDE
      # 4. Modern API unchanged (GET /store/inventory) -> IGNORE
      # 5. Modern API new method (PUT /store/order) -> INCLUDE
      # 6. Modern API modified (DELETE /store/order/{orderId}) -> INCLUDE
      # 7. New API (POST /products) -> INCLUDE (acts as broken/good toggle for rules)
      # 8. Test deep-ref (GET /inventory/check) -> INCLUDE (ensures component pruning works)
      # ========================================================
      - name: üß™ Run Robust Mix Test
        run: |
          echo "--- 1. Generating Diff ---"
          node -e "
            const fs = require('fs'); const diff = require('openapi-diff');
            const oldSpec = fs.readFileSync('tests/main.yaml', 'utf8');
            const newSpec = fs.readFileSync('tests/pr_robust_mix.yaml', 'utf8');
            diff.diffSpecs({ sourceSpec: { content: oldSpec, location: 'old', format: 'openapi3' }, destinationSpec: { content: newSpec, location: 'new', format: 'openapi3' } })
            .then(res => fs.writeFileSync('diff.json', JSON.stringify(res, null, 2)));
          "

          # Setup simulation files
          cp tests/pr_robust_mix.yaml swagger_head.yaml
          cp tests/main.yaml swagger_main.yaml

          echo "--- 2. Running Generator Script ---"
          python scripts/generate_partial_spec.py "tests/baseline.yaml"

          echo "üîç DEBUG: CONTENT OF PARTIAL_SPEC.JSON"
          cat partial_spec.json
          echo "--------------------------------------"

          echo "--- 3. Verifying Logic (Python Assertions) ---"
          python -c "
          import json, sys

          with open('partial_spec.json') as f:
              spec = json.load(f)
          paths = spec.get('paths', {})

          def check(path, method, should_exist, test_case):
              exists = path in paths and method in paths[path]
              status = '‚úÖ' if exists == should_exist else '‚ùå'
              state = 'PRESENT' if exists else 'ABSENT'
              expected = 'PRESENT' if should_exist else 'ABSENT'
              print(f'{status} TC{test_case}: {method.upper():6} {path:30} {state:8} (Expected: {expected})')
              if exists != should_exist:
                  print(f'   ‚ùå FAILED: Test Case {test_case} failed!')
                  sys.exit(1)

          print('\\n=== LEGACY API CHECKS (in baseline.yaml) ===')
          check('/pet', 'get', False, 1)                      # 1. Unchanged -> ABSENT
          check('/pet/findByStatus', 'get', False, 2)         # 2. Modified -> ABSENT (Ignored)
          check('/pet/findByStatus', 'patch', True, 3)        # 3. New Method -> PRESENT

          print('\\n=== MODERN API CHECKS (added after baseline) ===')
          check('/store/inventory', 'get', False, 4)          # 4. Unchanged -> ABSENT
          check('/store/order', 'put', True, 5)               # 5. New Method -> PRESENT
          check('/store/order/{orderId}', 'delete', True, 6)  # 6. Modified -> PRESENT

          print('\\n=== NEW API CHECKS (not in main.yaml) ===')
          check('/products', 'post', True, 7)                 # 7. New Path -> PRESENT
          check('/inventory/check', 'get', True, 8)           # 8. New Path with deep-ref -> PRESENT

          print('\\n‚úÖ ALL TEST CASES PASSED!')
          "

          echo "--- 4. Running Spectral (Expect FAILURE if bad code) ---"
          # The test should validate the spec is properly formed
          if npx spectral lint partial_spec.json --ruleset .spectral.yaml; then
            echo "‚úÖ SUCCESS: Spectral validation passed."
          else
            echo "‚ö†Ô∏è  Spectral found issues (as expected for this test)."
          fi


      # ---------------------------------------------------------
      # 5. Run Spectral (Linting)
      # ---------------------------------------------------------
      - name: üõ°Ô∏è Run Spectral Linting
        # '|| true' allows us to continue to the processing step even if errors are found
        run: npx spectral lint partial_spec.json --ruleset .spectral.yaml --format json --output spectral-results.json || true

      # ---------------------------------------------------------
      # 6. Process Results (Clean Annotations & Summary)
      # ---------------------------------------------------------
      - name: üìä Process Results
        shell: python
        run: |
          import json
          import os
          import sys

          # 1. Load Spectral JSON Results
          try:
              with open('spectral-results.json', 'r') as f:
                  results = json.load(f)
          except FileNotFoundError:
              results = []

          # 2. Prepare Markdown Header
          md_output = "## üõ°Ô∏è API Governance Results\n\n"
          
          if not results:
              md_output += "‚úÖ **No governance issues found!** The spec is clean.\n"
              with open(os.environ['GITHUB_STEP_SUMMARY'], 'a') as f:
                  f.write(md_output)
              sys.exit(0)

          # 3. Build Table & Annotations
          md_output += f"Found **{len(results)}** issues in `partial_spec.json`:\n\n"
          
          # --- UPDATED: Removed 'Line' Column from Table ---
          md_output += "| Severity | Rule | Message |\n"
          md_output += "| :---: | :--- | :--- |\n"

          # Severity: 0=Error, 1=Warn, 2=Info, 3=Hint
          sev_map = {0: "ERROR", 1: "WARN", 2: "INFO", 3: "HINT"}
          icon_map = {0: "üî¥", 1: "‚ö†Ô∏è", 2: "‚ÑπÔ∏è", 3: "üí°"}
          
          has_errors = False

          for r in results:
              severity = r.get('severity', 0)
              code = str(r.get('code', 'governance-issue')) # Ensure string
              message = r.get('message', '')
              source = r.get('source', 'partial_spec.json')
              line = r['range']['start']['line'] + 1
              
              # --- A. GITHUB ANNOTATIONS ---
              gh_level = "error" if severity == 0 else "warning"
              
              # We explicitly set title={code}. 
              # This forces the bold header to be the Rule Name (e.g. "path-params")
              # instead of the Step Name ("Process Results").
              print(f"::{gh_level} file={source},line={line},title={code}::{message}")

              # --- B. MARKDOWN SUMMARY ---
              icon = icon_map.get(severity, "üî¥")
              level_name = sev_map.get(severity, "ERROR")
              safe_msg = message.replace("|", "\|") 
              
              # Updated: Removed Line column from row
              md_output += f"| {icon} {level_name} | `{code}` | {safe_msg} |\n"

              if severity == 0:
                  has_errors = True

          # 4. Write Markdown to Job Summary
          with open(os.environ['GITHUB_STEP_SUMMARY'], 'a') as f:
              f.write(md_output)

          # 5. Fail the job if Critical Errors exist
          if has_errors:
              print("\n‚ùå Critical governance errors found.")
              sys.exit(0)
          else:
              print("\n‚úÖ Warnings found, but no critical errors.")
              sys.exit(0)